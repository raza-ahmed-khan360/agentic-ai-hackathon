"use strict";(globalThis.webpackChunkmy_ai_book=globalThis.webpackChunkmy_ai_book||[]).push([[316],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var t=i(6540);const r={},s=t.createContext(r);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(s.Provider,{value:n},e.children)}},9439:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter3","title":"chapter3","description":"Brilliant \u2014 I\u2019ll continue naturally in clean, structured British English, without Markdown restrictions, and produce the remaining three chapters so that the full textbook reaches five chapters.","source":"@site/docs/chapter3.md","sourceDirName":".","slug":"/chapter3","permalink":"/agentic-ai-hackathon/docs/chapter3","draft":false,"unlisted":false,"editUrl":"https://github.com/raza-ahmed-khan360/agentic-ai-hackathon/tree/main/docs/chapter3.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"**Chapter 2 \u2014 The Digital Twin (Gazebo Simulation)**","permalink":"/agentic-ai-hackathon/docs/chapter2"},"next":{"title":"**Chapter 4 \u2014 Motion Planning and Control**","permalink":"/agentic-ai-hackathon/docs/chapter4"}}');var r=i(4848),s=i(8453);const a={},o=void 0,l={},c=[{value:"3.1 The Role of Perception in Physical AI",id:"31-the-role-of-perception-in-physical-ai",level:2},{value:"3.2 Core Robotic Sensors",id:"32-core-robotic-sensors",level:2},{value:"Cameras (RGB and Depth)",id:"cameras-rgb-and-depth",level:3},{value:"LiDAR",id:"lidar",level:3},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:3},{value:"Force and Torque Sensors",id:"force-and-torque-sensors",level:3},{value:"Joint Encoders",id:"joint-encoders",level:3},{value:"3.3 The Perception Pipeline in ROS 2",id:"33-the-perception-pipeline-in-ros-2",level:2},{value:"1. Sensor Drivers",id:"1-sensor-drivers",level:3},{value:"2. Pre-processing",id:"2-pre-processing",level:3},{value:"3. Feature Extraction",id:"3-feature-extraction",level:3},{value:"4. Data Fusion",id:"4-data-fusion",level:3},{value:"5. Semantic Understanding",id:"5-semantic-understanding",level:3},{value:"6. Publishing to Downstream Systems",id:"6-publishing-to-downstream-systems",level:3}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["Brilliant \u2014 I\u2019ll continue naturally in clean, structured British English, without Markdown restrictions, and produce the remaining ",(0,r.jsx)(n.strong,{children:"three chapters"})," so that the full textbook reaches ",(0,r.jsx)(n.strong,{children:"five chapters"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Here are ",(0,r.jsx)(n.strong,{children:"Chapters 3, 4, and 5"}),", written at the same professional and educational level as the first two."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h1,{id:"chapter-3--sensors-and-the-perception-stack",children:(0,r.jsx)(n.strong,{children:"Chapter 3 \u2014 Sensors and the Perception Stack"})}),"\n",(0,r.jsx)(n.p,{children:"Robots require accurate awareness of their surroundings to behave intelligently in the physical world. Perception provides the foundation for localisation, navigation, manipulation, and high-level decision-making. This chapter explores the structure of the perception pipeline and how ROS 2 integrates data from multiple sensors."}),"\n",(0,r.jsx)(n.h2,{id:"31-the-role-of-perception-in-physical-ai",children:"3.1 The Role of Perception in Physical AI"}),"\n",(0,r.jsx)(n.p,{children:"Perception converts raw sensor readings into meaningful information. While raw data may include pixel intensities or ranges from a laser scanner, perception transforms these into actionable outputs:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The robot\u2019s position in the world"}),"\n",(0,r.jsx)(n.li,{children:"The 3D layout of the environment"}),"\n",(0,r.jsx)(n.li,{children:"Object identities and locations"}),"\n",(0,r.jsx)(n.li,{children:"Human pose and gesture"}),"\n",(0,r.jsx)(n.li,{children:"Contact forces and surface properties"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Unlike digital AI, which processes perfect images, physical robots must interpret noisy, incomplete, and ambiguous sensory data. This makes filtering, fusion, and probabilistic reasoning essential."}),"\n",(0,r.jsx)(n.h2,{id:"32-core-robotic-sensors",children:"3.2 Core Robotic Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Robots commonly use a mixture of the following sensors:"}),"\n",(0,r.jsx)(n.h3,{id:"cameras-rgb-and-depth",children:"Cameras (RGB and Depth)"}),"\n",(0,r.jsx)(n.p,{children:"Cameras provide rich information but require significant compute for processing. Depth cameras (structured light, stereo, or Time-of-Flight) add per-pixel distance measurement, essential for grasping and 3D mapping."}),"\n",(0,r.jsx)(n.h3,{id:"lidar",children:"LiDAR"}),"\n",(0,r.jsx)(n.p,{children:"LiDAR provides accurate range measurements with high angular precision, making it ideal for mapping and navigation. Modern multi-layer LiDARs give dense 3D pointclouds."}),"\n",(0,r.jsx)(n.h3,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,r.jsx)(n.p,{children:"The IMU provides acceleration, angular velocity, and sometimes orientation. It is essential for motion tracking, balancing, and sensor fusion."}),"\n",(0,r.jsx)(n.h3,{id:"force-and-torque-sensors",children:"Force and Torque Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Used at robot joints, grippers, and feet. These allow compliant motion, safe human\u2013robot interaction, and precise manipulation tasks."}),"\n",(0,r.jsx)(n.h3,{id:"joint-encoders",children:"Joint Encoders"}),"\n",(0,r.jsx)(n.p,{children:"These measure the position or velocity of robot joints and are crucial for forward and inverse kinematics."}),"\n",(0,r.jsx)(n.h2,{id:"33-the-perception-pipeline-in-ros-2",children:"3.3 The Perception Pipeline in ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"A standard ROS 2 perception stack includes the following stages:"}),"\n",(0,r.jsx)(n.h3,{id:"1-sensor-drivers",children:"1. Sensor Drivers"}),"\n",(0,r.jsx)(n.p,{children:"These nodes interface directly with hardware or simulation, publishing raw sensor data."}),"\n",(0,r.jsx)(n.h3,{id:"2-pre-processing",children:"2. Pre-processing"}),"\n",(0,r.jsx)(n.p,{children:"Noise removal, rectification, calibration, and depth filtering happen here."}),"\n",(0,r.jsx)(n.h3,{id:"3-feature-extraction",children:"3. Feature Extraction"}),"\n",(0,r.jsx)(n.p,{children:"Keypoints, descriptors, edges, and higher-level structures are extracted."}),"\n",(0,r.jsx)(n.h3,{id:"4-data-fusion",children:"4. Data Fusion"}),"\n",(0,r.jsx)(n.p,{children:"Techniques such as EKF (Extended Kalman Filter), UKF, and particle filters merge data from cameras, LiDAR, and IMUs."}),"\n",(0,r.jsx)(n.h3,{id:"5-semantic-understanding",children:"5. Semantic Understanding"}),"\n",(0,r.jsx)(n.p,{children:"Object detection, semantic segmentation, or 3D scene understanding via neural networks."}),"\n",(0,r.jsx)(n.h3,{id:"6-publishing-to-downstream-systems",children:"6. Publishing to Downstream Systems"}),"\n",(0,r.jsx)(n.p,{children:"Navigation, mapping, manipulation, and behaviour nodes consume perception outputs."}),"\n",(0,r.jsx)(n.p,{children:"This modular approach allows multiple algorithms to be swapped without changing the entire system."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);